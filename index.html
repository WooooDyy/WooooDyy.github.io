<!DOCTYPE HTML>
<html lang="en">
<head>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Zhiheng Xi (Â•öÂøóÊÅí)</title>

    <meta name="author" content="Zhiheng Xi">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
<!--    <link rel="icon"-->
<!--          href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">-->
    <link rel="icon"  href="images/bitbug_favicon.ico">
</head>

<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr style="padding:0px">
                    <td style="padding:2.5%;width:63%;vertical-align:middle">
                        <p class="name" style="text-align: center;">
                            Zhiheng Xi
                        </p>
                        <p>I am a third-year PhD student at <a href="https://nlp.fudan.edu.cn/">Fudan NLP Group</a>
                            of <a href="https://cs.fudan.edu.cn/">Computer School, Fudan University</a>,
                            where I work on natural language processing (NLP) and deep learning (DL). I am advised by
                            Prof. <a href="https://guitaowufeng.github.io/">Tao Gui</a>, Prof. <a
                                    href="http://qizhang.info/index.html">Qi Zhang</a>, and Prof. <a
                                    href="https://xuanjing-huang.github.io/">Xuanjing Huang</a>.
                            Previously, I got my bachelor's degree from <a href="https://www.nju.edu.cn/">Nanjing
                            University</a>, advised by Prof. Jia Liu.
                            I'm honored to be interning at <a href="https://www.shlab.org.cn/">Shanghai AI lab</a> currently.
                            In 2021, I completed a fantastic internship at <a href="https://azure.microsoft.com">Microsoft Azure</a>, advised by <a href="https://github.com/wujysh">Jiaye Wu</a> and Hang Zhang.
                            My research is supported by CIE-Tencent Doctoral Research Incentive Project (È¶ñÂ±ä‰∏≠ÂõΩÁîµÂ≠êÂ≠¶‰ºö‚ÄîËÖæËÆØÂçöÂ£´ÁîüÁßëÁ†îÊøÄÂä±ËÆ°Âàí(Ê∑∑ÂÖÉÂ§ßÊ®°Âûã‰∏ìÈ°π)).
                        </p>

                        <!-- <p>
                          At Google I've worked on <a href="https://www.google.com/glass/start/">Glass</a>,  <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">Jump</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, and <a href="https://www.matthewtancik.com/nerf">NeRF</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I've received the <a href="https://www2.eecs.berkeley.edu/Students/Awards/15/">C.V. Ramamoorthy Distinguished Research Award</a> and the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.
                        </p> -->
                        <p style="text-align:center">
                            <a href="mailto:zhxi22@m.fudan.edu.cn">Email</a> &nbsp;/&nbsp;
                            <!-- <a href="TODO">CV</a> &nbsp;/&nbsp; -->
                            <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                            <a href="https://scholar.google.com.hk/citations?user=zSVLkqAAAAAJ">Google Scholar</a>
                            &nbsp;/&nbsp;
                            <a href="https://github.com/WooooDyy">Github</a> &nbsp;/&nbsp;
                            <a href="https://x.com/Be1ong1">Twitter</a> &nbsp;/&nbsp;
                            <a href="https://www.zhihu.com/people/fang-kong-43">Zhihu</a>
                            
                        </p>
                    </td>
                    <td style="padding:2.5%;width:40%;max-width:40%">
                        <a href="images/2023ÁîüÊó•.jpg"><img style="width:80%;max-width:100%" alt="profile photo"
                                                            src="images/2023ÁîüÊó•.jpg" class="hoverZoomLink"></a>
                    </td>
                </tr>
                </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Research</h2>
                        <p>
                            I have general interest in deep learning, natural language processing, and robust machine
                            learning. Recently, I focus my research on large language models (LLMs), LLM reasoning, LLM-based
                            agents, and LLM Alignment.
                            <!-- Representative papers are <span class="highlight">highlighted</span>. -->
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>



            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>

                    <tr onmouseout="agentgymrl_stop()" onmouseover="agentgymrl_start()">
                        <!-- <td style="padding:20px;width:30%;vertical-align:middle">
                            <div class="one">
                                <div class="two" id='mathcritique_image.jpg'>
                                    <img src='images/mathcritique_image.jpg' width="190">
                                </div>
                                <img src='images/mathcritique_image.jpg' width="190">
                            </div>
                            <script type="text/javascript">
                                function mathcritique_start() {
                                    document.getElementById('mathcritique_image').style.opacity = "1";
                                }
    
                                function mathcritique_stop() {
                                    document.getElementById('mathcritique_image').style.opacity = "0";
                                }
    
                                mathcritique_stop()
                            </script>
                        </td> -->
                        <td style="padding:20px;width:100%;vertical-align:middle">
                            <a href="https://arxiv.org/abs/2406.04151">
                                <span class="papertitle">AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning</span>
                            </a>
                            <br>
                            <strong>Zhiheng Xi</strong>, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, Wei He, Yiwen Ding, Guanyu Li, Zehui Chen, Zhengyin Du, Xuesong Yao, Yufei Xu, Jiecao Chen, Tao Gui, Zuxuan Wu, Qi Zhang, Xuanjing Huang, Yu-Gang Jiang
                            <br>
                            <em>Preprint.</em> Sep, 2025
    
                            <br>
                            <a href="https://AgentGym-RL.github.io/">project page</a> /
                            <a href="https://github.com/woooodyy/AgentGym-RL">codes</a> /
                            <a href="https://arxiv.org/abs/2406.04151">paper</a> /
                            <!-- <a href="https://huggingface.co/datasets/MathCritique/MathCritique-76k">dataset</a>  -->
    
                            <!-- <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a> -->
    
    
                            <!-- <p></p> -->
                            <!-- <p> -->
                                <!-- Training large language models (LLMs) to spend more time thinking and reflection before responding is crucial for effectively solving complex reasoning tasks in fields such as science, coding, and mathematics. However, the effectiveness of mechanisms like self-reflection and self-correction depends on the model's capacity to accurately assess its own performance, which can be limited by factors such as initial accuracy, question difficulty, and the lack of external feedback. In this paper, we delve into a two-player paradigm that separates the roles of reasoning and critique models, where the critique model provides step-level feedback to supervise the reasoning (actor) model during both test-time and training-time. We first propose AutoMathCritique, an automated and scalable framework for collecting critique data, resulting in a dataset of $76,321$ responses paired with step-level feedback. Fine-tuning language models with this dataset enables them to generate natural language feedback for mathematical reasoning. We demonstrate that the critique models consistently improve the actor's performance on difficult queries at test-time, especially when scaling up inference-time computation. Motivated by these findings, we introduce the critique-based supervision to the actor's self-training process, and propose a critique-in-the-loop self-improvement method. Experiments show that the method improves the actor's exploration efficiency and solution diversity, especially on challenging queries, leading to a stronger reasoning model. Lastly, we take the preliminary step to explore training self-talk reasoning models via critique supervision and showcase their potential. -->
    
                            <!-- </p> -->
                            <!-- <p>
                            </p> -->
                        </td>
                    </tr>

                                    <tr onmouseout="agentgym_stop()" onmouseover="agentgym_start()">
                    <!-- <td style="padding:20px;width:30%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='agentgym_image'>
                                <img src='images/agentgym_image.jpg' width="190">
                            </div>
                            <img src='images/agentgym_image.jpg' width="190">
                        </div>
                        <script type="text/javascript">
                            function agentgym_start() {
                                document.getElementById('agentgym_image').style.opacity = "1";
                            }

                            function agentgym_stop() {
                                document.getElementById('agentgym_image').style.opacity = "0";
                            }

                            agentgym_stop()
                        </script>
                    </td> -->
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2406.04151">
                            <span class="papertitle">AgentGym: Evolving Large Language Model-based Agents across Diverse Environments</span>
                        </a>
                        <br>
                        <strong>Zhiheng Xi</strong>, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, Songyang Gao, Lu Chen, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, Yu-Gang Jiang

                        <br>
                        <em>ACL 2025</em>. Preprint at June, 2024.

                        <br>
                        <a href="https://agentgym.github.io/">project page</a> /
                        <a href="https://github.com/WooooDyy/AgentGym">codes and platform</a> /
                        <a href="https://arxiv.org/abs/2406.04151">paper</a> /
                        <a href="https://huggingface.co/datasets/AgentGym/AgentTraj-L">dataset</a> /
                        <a href="https://huggingface.co/datasets/AgentGym/AgentEval">benchmark</a> /
                        <a href="https://huggingface.co/AgentGym/AgentEvol-7B">model</a>

                        <!-- <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a> -->


                        <!-- <p></p> -->
                        <!-- <p> -->
<!--                            In this paper, we take the first step towards building generally-capable LLM-based agents with self-evolution ability.-->
                            <!-- We propose <strong>AgentGym</strong>, a framework designed to help the community easily evaluate and develop generally-capable LLM-based agents. It features diverse interactive environments and tasks with a unified format. It supports real-time feedback and concurrency, and is easily scalable. -->
                            <!-- It also includes a high-quality trajectory set <strong>AgentTraj</strong> and a benchmark suite <strong>AgentEval</strong>. -->
<!--                            In this paper, we propose <strong>\(R^3\)</strong>:-->
<!--                            Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL),-->
<!--                            a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models.-->

                        <!-- </p> -->
                        <!-- <p> -->
                            <!-- To study the evolution potential of general LLM-based agents, we propose a novel method, <strong>AgentEvol</strong>, to investigate the potential of agent self-evolution beyond previously seen data across tasks and environments. -->
                        <!-- </p> -->
                        <!-- <p></p> -->
                    </td>
                </tr>

                    <tr onmouseout="bmmr_stop()" onmouseover="bmmr_start()">
                        <!-- <td style="padding:20px;width:30%;vertical-align:middle">
                            <div class="one">
                                <div class="two" id='mathcritique_image.jpg'>
                                    <img src='images/mathcritique_image.jpg' width="190">
                                </div>
                                <img src='images/mathcritique_image.jpg' width="190">
                            </div>
                            <script type="text/javascript">
                                function mathcritique_start() {
                                    document.getElementById('mathcritique_image').style.opacity = "1";
                                }
    
                                function mathcritique_stop() {
                                    document.getElementById('mathcritique_image').style.opacity = "0";
                                }
    
                                mathcritique_stop()
                            </script>
                        </td> -->
                        <td style="padding:20px;width:100%;vertical-align:middle">
                            <a href="https://arxiv.org/abs/2507.03483">
                                <span class="papertitle">BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning Dataset</span>
                            </a>
                            <br>
                            <strong>Zhiheng Xi</strong>, Guanyu Li, Yutao Fan, Honglin Guo, Yufang Liu, Xiaoran Fan, Jiaqi Liu, Jingchao Ding, Wangmeng Zuo, Zhenfei Yin, Lei Bai, Tao Ji, Tao Gui, Qi Zhang, Philip Torr, Xuanjing Huang
                            <br>
                            <em>Preprint.</em> July, 2025
    
                            <br>
                            <a href="https://bmmr.pages.dev/">project page</a> /
                            <a href="https://github.com/woooodyy/BMMR">codes</a> /
                            <a href="https://arxiv.org/abs/2507.03483">paper</a> /
                            <a href="https://huggingface.co/datasets/guanyu615/BMMR">dataset</a> 
    
                            <!-- <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a> -->
    
    
                            <!-- <p></p> -->
                            <!-- <p> -->
                                <!-- Training large language models (LLMs) to spend more time thinking and reflection before responding is crucial for effectively solving complex reasoning tasks in fields such as science, coding, and mathematics. However, the effectiveness of mechanisms like self-reflection and self-correction depends on the model's capacity to accurately assess its own performance, which can be limited by factors such as initial accuracy, question difficulty, and the lack of external feedback. In this paper, we delve into a two-player paradigm that separates the roles of reasoning and critique models, where the critique model provides step-level feedback to supervise the reasoning (actor) model during both test-time and training-time. We first propose AutoMathCritique, an automated and scalable framework for collecting critique data, resulting in a dataset of $76,321$ responses paired with step-level feedback. Fine-tuning language models with this dataset enables them to generate natural language feedback for mathematical reasoning. We demonstrate that the critique models consistently improve the actor's performance on difficult queries at test-time, especially when scaling up inference-time computation. Motivated by these findings, we introduce the critique-based supervision to the actor's self-training process, and propose a critique-in-the-loop self-improvement method. Experiments show that the method improves the actor's exploration efficiency and solution diversity, especially on challenging queries, leading to a stronger reasoning model. Lastly, we take the preliminary step to explore training self-talk reasoning models via critique supervision and showcase their potential. -->
    
                            <!-- </p> -->
                            <!-- <p>
                            </p> -->
                        </td>
                    </tr>

                    <tr onmouseout="mathcritique_stop()" onmouseover="mathcritique_start()">
                        <!-- <td style="padding:20px;width:30%;vertical-align:middle">
                            <div class="one">
                                <div class="two" id='mathcritique_image.jpg'>
                                    <img src='images/mathcritique_image.jpg' width="190">
                                </div>
                                <img src='images/mathcritique_image.jpg' width="190">
                            </div>
                            <script type="text/javascript">
                                function mathcritique_start() {
                                    document.getElementById('mathcritique_image').style.opacity = "1";
                                }
    
                                function mathcritique_stop() {
                                    document.getElementById('mathcritique_image').style.opacity = "0";
                                }
    
                                mathcritique_stop()
                            </script>
                        </td> -->
                        <td style="padding:20px;width:100%;vertical-align:middle">
                            <a href="https://arxiv.org/abs/2411.16579">
                                <span class="papertitle">Enhancing LLM Reasoning via Critique Models with Test-Time and Training-Time Supervision</span>
                            </a>
                            <br>
                            <strong>Zhiheng Xi</strong>, Dingwen Yang, Jixuan Huang, Jiafu Tang, Guanyu Li, Yiwen Ding, Wei He, Boyang Hong, Shihan Do, Wenyu Zhan, Xiao Wang, Rui Zheng, Tao Ji, Xiaowei Shi, Yitao Zhai, Rongxiang Weng, Jingang Wang, Xunliang Cai, Tao Gui, Zuxuan Wu, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Yu-Gang Jiang
                            <br>
                            <em>Preprint.</em> Nov, 2024
    
                            <br>
                            <a href="https://mathcritique.github.io/">project page</a> /
                            <a href="https://github.com/WooooDyy/MathCritique">codes</a> /
                            <a href="https://arxiv.org/abs/2411.16579">paper</a> /
                            <a href="https://huggingface.co/datasets/MathCritique/MathCritique-76k">dataset</a> 
    
                            <!-- <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a> -->
    
    
                            <!-- <p></p> -->
                            <!-- <p> -->
                                <!-- Training large language models (LLMs) to spend more time thinking and reflection before responding is crucial for effectively solving complex reasoning tasks in fields such as science, coding, and mathematics. However, the effectiveness of mechanisms like self-reflection and self-correction depends on the model's capacity to accurately assess its own performance, which can be limited by factors such as initial accuracy, question difficulty, and the lack of external feedback. In this paper, we delve into a two-player paradigm that separates the roles of reasoning and critique models, where the critique model provides step-level feedback to supervise the reasoning (actor) model during both test-time and training-time. We first propose AutoMathCritique, an automated and scalable framework for collecting critique data, resulting in a dataset of $76,321$ responses paired with step-level feedback. Fine-tuning language models with this dataset enables them to generate natural language feedback for mathematical reasoning. We demonstrate that the critique models consistently improve the actor's performance on difficult queries at test-time, especially when scaling up inference-time computation. Motivated by these findings, we introduce the critique-based supervision to the actor's self-training process, and propose a critique-in-the-loop self-improvement method. Experiments show that the method improves the actor's exploration efficiency and solution diversity, especially on challenging queries, leading to a stronger reasoning model. Lastly, we take the preliminary step to explore training self-talk reasoning models via critique supervision and showcase their potential. -->
    
                            <!-- </p> -->
                            <!-- <p>
                            </p> -->
                        </td>
                    </tr>


                    <tr onmouseout="GSI_stop()" onmouseover="GSI_start()">
                        <!-- <td style="padding:20px;width:30%;vertical-align:middle">
                            <div class="one">
                                <div class="two" id='mathcritique_image.jpg'>
                                    <img src='images/mathcritique_image.jpg' width="190">
                                </div>
                                <img src='images/mathcritique_image.jpg' width="190">
                            </div>
                            <script type="text/javascript">
                                function mathcritique_start() {
                                    document.getElementById('mathcritique_image').style.opacity = "1";
                                }
    
                                function mathcritique_stop() {
                                    document.getElementById('mathcritique_image').style.opacity = "0";
                                }
    
                                mathcritique_stop()
                            </script>
                        </td> -->
                        <td style="padding:20px;width:100%;vertical-align:middle">
                            <a href="https://arxiv.org/abs/2411.16579">
                                <span class="papertitle">Mitigating Tail Narrowing in LLM Self-Improvement via Socratic-Guided Sampling</span>
                            </a>
                            <br>
                            Yiwen Ding*, <strong>Zhiheng Xi* (Co-first Author)</strong>, Wei He, Zhuoyuan Li, Yitao Zhai, Xiaowei Shi, Xunliang Cai, Tao Gui, Qi Zhang, Xuanjing Huang       <br>
                            <em>NAACL 2024</em>
    
                            <br>
                            <!-- <a href="https://mathcritique.github.io/">project page</a> / -->
                            <a href="https://github.com/Yiwen-Ding/Guided-Self-Improvement">codes</a> /
                            <a href="https://arxiv.org/abs/2411.00750">paper</a> 
                            <!-- <a href="https://huggingface.co/datasets/MathCritique/MathCritique-76k">dataset</a> / -->
    
                            <!-- <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a> -->
    
    
                            <!-- <p></p> -->
                            <!-- <p> -->
                                <!-- Training large language models (LLMs) to spend more time thinking and reflection before responding is crucial for effectively solving complex reasoning tasks in fields such as science, coding, and mathematics. However, the effectiveness of mechanisms like self-reflection and self-correction depends on the model's capacity to accurately assess its own performance, which can be limited by factors such as initial accuracy, question difficulty, and the lack of external feedback. In this paper, we delve into a two-player paradigm that separates the roles of reasoning and critique models, where the critique model provides step-level feedback to supervise the reasoning (actor) model during both test-time and training-time. We first propose AutoMathCritique, an automated and scalable framework for collecting critique data, resulting in a dataset of $76,321$ responses paired with step-level feedback. Fine-tuning language models with this dataset enables them to generate natural language feedback for mathematical reasoning. We demonstrate that the critique models consistently improve the actor's performance on difficult queries at test-time, especially when scaling up inference-time computation. Motivated by these findings, we introduce the critique-based supervision to the actor's self-training process, and propose a critique-in-the-loop self-improvement method. Experiments show that the method improves the actor's exploration efficiency and solution diversity, especially on challenging queries, leading to a stronger reasoning model. Lastly, we take the preliminary step to explore training self-talk reasoning models via critique supervision and showcase their potential. -->
    
                            <!-- </p> -->
                            <!-- <p>
                            </p> -->
                        </td>
                    </tr>

                    <tr onmouseout="ReachQA_stop()" onmouseover="ReachQA_start()">
                        <!-- <td style="padding:20px;width:30%;vertical-align:middle">
                            <div class="one">
                                <div class="two" id='mathcritique_image.jpg'>
                                    <img src='images/mathcritique_image.jpg' width="190">
                                </div>
                                <img src='images/mathcritique_image.jpg' width="190">
                            </div>
                            <script type="text/javascript">
                                function mathcritique_start() {
                                    document.getElementById('mathcritique_image').style.opacity = "1";
                                }
    
                                function mathcritique_stop() {
                                    document.getElementById('mathcritique_image').style.opacity = "0";
                                }
    
                                mathcritique_stop()
                            </script>
                        </td> -->
                        <td style="padding:20px;width:100%;vertical-align:middle">
                            <a href="https://arxiv.org/abs/2410.18798">
                                <span class="papertitle">Distill Visual Chart Reasoning Ability from LLMs to MLLMs
                                </span>
                            </a>
                            <br>
                            Wei He*, <strong>Zhiheng Xi* (Co-first Author)</strong>, Wanxu Zhao*, Xiaoran Fan, Yiwen Ding, Zifei Shan, Tao Gui, Qi Zhang, Xuanjing Huang                           
                            <em>EMNLP 2025.</em>
    
                            <br>
                            <!-- <a href="https://mathcritique.github.io/">project page</a> / -->
                            <a href="https://github.com/hewei2001/ReachQA">codes</a> /
                            <a href="https://arxiv.org/abs/2410.18798">paper</a> /
                            <a href="https://huggingface.co/datasets/hewei2001/ReachQA">dataset</a> 
    
                            <!-- <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a> -->
    
    
                            <!-- <p></p> -->
                            <!-- <p> -->
                                <!-- Training large language models (LLMs) to spend more time thinking and reflection before responding is crucial for effectively solving complex reasoning tasks in fields such as science, coding, and mathematics. However, the effectiveness of mechanisms like self-reflection and self-correction depends on the model's capacity to accurately assess its own performance, which can be limited by factors such as initial accuracy, question difficulty, and the lack of external feedback. In this paper, we delve into a two-player paradigm that separates the roles of reasoning and critique models, where the critique model provides step-level feedback to supervise the reasoning (actor) model during both test-time and training-time. We first propose AutoMathCritique, an automated and scalable framework for collecting critique data, resulting in a dataset of $76,321$ responses paired with step-level feedback. Fine-tuning language models with this dataset enables them to generate natural language feedback for mathematical reasoning. We demonstrate that the critique models consistently improve the actor's performance on difficult queries at test-time, especially when scaling up inference-time computation. Motivated by these findings, we introduce the critique-based supervision to the actor's self-training process, and propose a critique-in-the-loop self-improvement method. Experiments show that the method improves the actor's exploration efficiency and solution diversity, especially on challenging queries, leading to a stronger reasoning model. Lastly, we take the preliminary step to explore training self-talk reasoning models via critique supervision and showcase their potential. -->
    
                            <!-- </p> -->
                            <!-- <p>
                            </p> -->
                        </td>
                    </tr>





                <tr onmouseout="RMB_stop()" onmouseover="RMB_start()">
                    <!-- <td style="padding:20px;width:30%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='mathcritique_image.jpg'>
                                <img src='images/mathcritique_image.jpg' width="190">
                            </div>
                            <img src='images/mathcritique_image.jpg' width="190">
                        </div>
                        <script type="text/javascript">
                            function mathcritique_start() {
                                document.getElementById('mathcritique_image').style.opacity = "1";
                            }

                            function mathcritique_stop() {
                                document.getElementById('mathcritique_image').style.opacity = "0";
                            }

                            mathcritique_stop()
                        </script>
                    </td> -->
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2410.09893">
                            <span class="papertitle">RMB: Comprehensively Benchmarking Reward Models in LLM Alignment

                            </span>
                        </a>
                        <br>
                        Enyu Zhou*, Guodong Zheng*, Binghai Wang*, <strong>Zhiheng Xi</strong>, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, Rui Zheng, Tao Gui, Qi Zhang, Xuanjing Huang                           
                        

                        <br>
                        <em>ICLR 2025</em>
                        <br>
                        <!-- <a href="https://mathcritique.github.io/">project page</a> / -->
                        <a href="https://github.com/Zhou-Zoey/RMB-Reward-Model-Benchmark">codes</a> /
                        <a href="https://arxiv.org/abs/2410.09893">paper</a> 
                        <!-- <a href="https://huggingface.co/datasets/hewei2001/ReachQA">dataset</a> / -->

                        <!-- <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a> -->


                        <!-- <p></p> -->
                        <!-- <p> -->
                            <!-- Training large language models (LLMs) to spend more time thinking and reflection before responding is crucial for effectively solving complex reasoning tasks in fields such as science, coding, and mathematics. However, the effectiveness of mechanisms like self-reflection and self-correction depends on the model's capacity to accurately assess its own performance, which can be limited by factors such as initial accuracy, question difficulty, and the lack of external feedback. In this paper, we delve into a two-player paradigm that separates the roles of reasoning and critique models, where the critique model provides step-level feedback to supervise the reasoning (actor) model during both test-time and training-time. We first propose AutoMathCritique, an automated and scalable framework for collecting critique data, resulting in a dataset of $76,321$ responses paired with step-level feedback. Fine-tuning language models with this dataset enables them to generate natural language feedback for mathematical reasoning. We demonstrate that the critique models consistently improve the actor's performance on difficult queries at test-time, especially when scaling up inference-time computation. Motivated by these findings, we introduce the critique-based supervision to the actor's self-training process, and propose a critique-in-the-loop self-improvement method. Experiments show that the method improves the actor's exploration efficiency and solution diversity, especially on challenging queries, leading to a stronger reasoning model. Lastly, we take the preliminary step to explore training self-talk reasoning models via critique supervision and showcase their potential. -->

                        <!-- </p> -->
                        <!-- <p>
                        </p> -->
                    </td>
                </tr>




                <tr onmouseout="R3_stop()" onmouseover="R3_start()">
                    <!-- <td style="padding:20px;width:30%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='R3_image'>
                                <img src='images/R3_image.jpg' width="190">
                            </div>
                            <img src='images/R3_image.jpg' width="190">
                        </div>
                        <script type="text/javascript">
                            function R3_start() {
                                document.getElementById('R3_image').style.opacity = "1";
                            }

                            function R3_stop() {
                                document.getElementById('R3_image').style.opacity = "0";
                            }

                            R3_stop()
                        </script>
                    </td> -->
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2402.05808">
                            <span class="papertitle">Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning</span>
                        </a>
                        <br>
                        <strong>Zhiheng Xi</strong>, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, Honglin Guo, Wei Shen, Xiaoran Fan, Yuhao Zhou, Shihan Dou, Xiao Wang, Xinbo Zhang, Peng Sun, Tao Gui, Qi Zhang, Xuanjing Huang

                        <br>
                        <!--          preprint.-->
                        <!--           <em>ICCV</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>-->
                        <em>ICML 2024; CIPS-LMG 2024 Outstanding Poster</em>

                        <br>
                        <a href="https://github.com/WooooDyy/LLM-Reverse-Curriculum-RL">codes</a>
                        /
                        <!-- <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a> -->

                        <a href="https://arxiv.org/abs/2402.05808">paper</a>
                        <!-- <p></p> -->
                        <!-- <p>

                            In this paper, we propose <strong>\(R^3\)</strong>:
                            Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL),
                            a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models.

                        </p> -->
                    </td>
                </tr>


                <!-- <tr onmouseout="llm_agent_survey_stop()" onmouseover="llm_agent_survey_start()"  bgcolor="#ffffd0"> -->
                <tr onmouseout="llm_agent_survey_stop()" onmouseover="llm_agent_survey_start()">
                    <!-- <td style="padding:20px;width:30%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='llm_agent_survey_image'>
                                <img src='images/llm_agent_survey.jpg' width="190">
                            </div>
                            <img src='images/llm_agent_survey.jpg' width="190">
                        </div>
                        <script type="text/javascript">
                            function llm_agent_survey_start() {
                                document.getElementById('llm_agent_survey_image').style.opacity = "1";
                            }

                            function llm_agent_survey_stop() {
                                document.getElementById('llm_agent_survey_image').style.opacity = "0";
                            }

                            llm_agent_survey_stop()
                        </script>
                    </td> -->
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2309.07864">
                            <span class="papertitle">The Rise and Potential of Large Language Model Based Agents: A Survey</span>
                        </a>
                        <br>
                        <strong>Zhiheng Xi</strong>, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming
                        Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Qin
                        Liu, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan
                        Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing
                        Huang, Tao Gui
                        <br>
                        <!--          preprint.-->
                        <!--           <em>ICCV</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>-->
                        <em>SCIENCE CHINA Information Sciences (SCIS), Cover Paper of SCIS Volume 68, Number 2, February 2025.</em> 
                        <!-- <br>Preprint at September, 2023 -->

                        <br>
                        <a href="https://github.com/WooooDyy/LLM-Agent-Paper-List">project page</a>
                        /
                        <!-- <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a> -->

                        <a href="https://arxiv.org/abs/2309.07864">paper</a>
                        <!-- <p></p> -->
                        <!-- <p> -->

                            <!-- In this paper, we provide a comprehensive survey of LLM-based agents with <strong>86 -->
                            <!-- pages</strong>. -->
                            <!-- We start by tracing the concept of agents from its philosophical origins to its development -->
                            <!-- in AI. -->
                            <!-- Next, the main body includes the construction of LLM-based agents, its extensive -->
                            <!-- applications, and the essential concept of Agent society. -->
                            <!--          Next we present a conceptual framework for LLM-based agents, comprising three main components: brain, perception, and action.-->
                            <!--          Subsequently, we explore the applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation.-->
                            <!--          Following this, we delve into agent societies-->
                            <!-- Finally, we discuss a range of key topics and open problems within the field, e.g., scaling -->
                            <!-- number of agentsand Agent-as-a-service. -->

                        <!-- </p> -->
                    </td>
                </tr>




                <tr onmouseout="self_polish_stop()" onmouseover="self_polish_start()">
                    <!-- <td style="padding:20px;width:30%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='self_polish_image'>
                                <img src='images/self_polish.jpg' width="190">
                            </div>
                            <img src='images/self_polish.jpg' width="190">
                        </div>
                        <script type="text/javascript">
                            function self_polish_start() {
                                document.getElementById('self_polish_image').style.opacity = "1";
                            }

                            function self_polish_stop() {
                                document.getElementById('self_polish_image').style.opacity = "0";
                            }

                            self_polish_stop()
                        </script>
                    </td> -->
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2305.14497">
                            <span class="papertitle">Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement</span>
                        </a>
                        <br>
                        <strong>Zhiheng Xi</strong>, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao, Jia Liu, Tao Gui,
                        Qi Zhang, Xuanjing Huang
                        <br>
                        <em>EMNLP 2023 Findings.</em>

                        <!-- <em>ICCV</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> -->
                        <br>
                        <a href="https://github.com/WooooDyy/Self-Polish">codes</a>
                        /
                        <!-- <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a> -->

                        <a href="https://arxiv.org/abs/2305.14497">paper</a>
                        <!-- <p></p> -->
                        <!-- <p> -->
                            <!-- Different from previous work like Chain-of-Thought (CoT) which enhance LLMs' reasoning -->
                            <!-- performance from the <strong> answer/reasoning side </strong>, -->
                            <!-- we start from the <strong> problem side </strong> and propose Self-Polish (SP). -->
                            <!-- It is a novel method that facilitates the model‚Äôs reasoning by guiding it to progressively -->
                            <!-- refine the given problems to be more comprehensible and solvable. -->
                        <!-- </p> -->
                    </td>
                </tr>


                <tr onmouseout="copate_stop()" onmouseover="copate_start()">
                    <!-- <td style="padding:20px;width:30%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='copate_image'>
                                <img src='images/copate.jpg' width="190">
                            </div>
                            <img src='images/copate.jpg' width="190">
                        </div>
                        <script type="text/javascript">
                            function copate_start() {
                                document.getElementById('copate_image').style.opacity = "1";
                            }

                            function copate_stop() {
                                document.getElementById('copate_image').style.opacity = "0";
                            }

                            copate_stop()
                        </script>
                    </td> -->
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://aclanthology.org/2023.findings-acl.759/">
                            <span class="papertitle">Connectivity Patterns are Task Embeddings</span>
                        </a>
                        <br>
                        <strong>Zhiheng Xi</strong>, Rui Zheng, Yuansen Zhang, XuanJing Huang, Zhongyu Wei, Minlong
                        Peng, Mingming Sun, Qi Zhang, Tao Gui
                        <br>
                        <em>ACL 2023 Findings.</em>

                        <!-- <em>ICCV</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> -->
                        <br>
                        <a href="https://github.com/WooooDyy/CoPaTE">codes</a>
                        /
                        <!-- <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a> -->

                        <a href="https://aclanthology.org/2023.findings-acl.759/">paper</a>
                        <!-- <p></p> -->
                        <!-- <p> -->
                            <!-- In this work, we draw inspiration from the operating mechanism of deep neural networks -->
                            <!-- (DNNs) and biological brains, -->
                            <!-- where neuronal activations are sparse and task-specific, -->
                            <!-- and we use the connectivity patterns of neurons as a unique identifier (<strong>Task -->
                            <!-- Embeddings</strong>) associated with the task. -->
                            <!-- Experiments show that our method consistently outperforms other baselines in predicting -->
                            <!-- inter-task transferability across data regimes and transfer settings, -->
                            <!-- while keeping high efficiency in computation and storage. -->
                        <!-- </p> -->
                    </td>
                </tr>

                <tr onmouseout="early_robust_stop()" onmouseover="early_robust_start()">
                    <!-- <td style="padding:20px;width:30%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='early_robust_image'>
                                <img src='images/early_robust.jpg' width="190">
                            </div>
                            <img src='images/early_robust.jpg' width="190">
                        </div>
                        <script type="text/javascript">
                            function early_robust_start() {
                                document.getElementById('early_robust_image').style.opacity = "1";
                            }

                            function early_robust_stop() {
                                document.getElementById('early_robust_image').style.opacity = "0";
                            }

                            early_robust_stop()
                        </script>
                    </td> -->
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://aclanthology.org/2022.emnlp-main.569/">
                            <span class="papertitle">Efficient Adversarial Training with Robust Early-Bird Tickets</span>
                        </a>
                        <br>
                        <strong>Zhiheng Xi</strong>, Rui Zheng, Tao Gui, Qi Zhang, Xuanjing Huang
                        <br>
                        <em>EMNLP 2022.</em>

                        <!-- <em>ICCV</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> -->
                        <br>
                        <a href="https://github.com/WooooDyy/early_robust">codes</a>
                        /
                        <!-- <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a> -->

                        <a href="https://aclanthology.org/2022.emnlp-main.569/">paper</a>
                        <!-- <p></p> -->
                        <!-- <p> -->
                            <!-- Adversarial training, a strong algorithm to enhance model robustness, is typically more -->
                            <!-- expensive than traditional fine-tuning because of the necessity to generate adversarial -->
                            <!-- examples via gradient descent. -->
                            <!-- Delving into the optimization process of adversarial training, -->
                            <!-- we find that robust connectivity patterns emerge in the early training phase (typically -->
                            <!-- 0.15~0.3 epochs), far before parameters converge. -->
                            <!-- Inspired by this finding, we dig out <strong>robust early-bird tickets (i.e., -->
                            <!-- subnetworks)</strong> to develop an efficient adversarial training method. -->
                        <!-- </p> -->
                    </td>
                </tr>

                <tr onmouseout="robust_data_stop()" onmouseover="robust_data_start()">
                    <!-- <td style="padding:20px;width:30%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='robust_data_image'>
                                <img src='images/robust_data.jpg' width="190">
                            </div>
                            <img src='images/robust_data.jpg' width="190">
                        </div>
                        <script type="text/javascript">
                            function robust_data_start() {
                                document.getElementById('robust_data_image').style.opacity = "1";
                            }

                            function robust_data_stop() {
                                document.getElementById('robust_data_image').style.opacity = "0";
                            }

                            robust_data_stop()
                        </script>
                    </td> -->
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://aclanthology.org/2023.findings-acl.146/">
                            <span class="papertitle">Characterizing the Impacts of Instances on Robustness</span>
                        </a>
                        <br>
                        Rui Zheng*, <strong>Zhiheng Xi* (Co-first Author)</strong>, Qin Liu, Wenbin Lai, Tao Gui, Qi
                        Zhang, Xuanjing Huang, Jin Ma, Ying Shan, Weifeng Ge
                        <br>
                        <em>ACL 2023 Findings.</em>

                        <!-- <em>ICCV</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> -->
                        <br>
                        <a href="https://github.com/WooooDyy/robust_data">codes</a>
                        /
                        <!-- <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a> -->

                        <a href="https://aclanthology.org/2023.findings-acl.146/">paper</a>
                        <!-- <p></p>
                        <p>

                            In this paper, we show that robust and non-robust instances in the training dataset,
                            though are both important for test performance, have contrary impacts on robustness,
                            which makes it possible to build a highly robust model by leveraging the training dataset in a more effective way.
                            We propose a new method that can distinguish between robust instances from non-robust ones according to the model‚Äôs sensitivity
                            to perturbations on individual instances during training.
                            Surprisingly, we find that the model under standard training easily overfits the robust instances
                            by relying on their simple patterns before the model completely learns their robust features.
                            Finally, we propose a new mitigation algorithm to further release the potential of robust instances.
                        </p> -->
                    </td>
                </tr>

                <tr onmouseout="group_invariant_alignment_stop()" onmouseover="group_invariant_alignment_start()">
                    <!-- <td style="padding:20px;width:30%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='group_invariant_alignment_image'>
                                <img src='images/group_invariant_alignment.jpg' width="190">
                            </div>
                            <img src='images/group_invariant_alignment.jpg' width="190">
                        </div>
                        <script type="text/javascript">
                            function group_invariant_alignment_start() {
                                document.getElementById('group_invariant_alignment_image').style.opacity = "1";
                            }

                            function group_invariant_alignment_stop() {
                                document.getElementById('group_invariant_alignment_image').style.opacity = "0";
                            }

                            group_invariant_alignment_stop()
                        </script>
                    </td> -->
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2310.11971">
                            <span class="papertitle">Improving Generalization of Alignment with Human Preferences through Group Invariant Learning</span>
                        </a>
                        <br>
                        Rui Zheng, Wei Shen, Yuan Hua, Wenbin Lai, Shihan Dou, Yuhao Zhou, <strong>Zhiheng Xi</strong>, Xiao Wang, Haoran Huang, Tao Gui, Qi Zhang, Xuanjing Huang

                        <br>
                        <em>ICLR 2024 (Spotlight).</em>
                        <br>
                        <a href="https://arxiv.org/abs/2310.11971">codes</a>
                        /
                        <a href="https://arxiv.org/abs/2310.11971">paper</a>
                        <!-- <p></p>
                        <p>
                        In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the established groups, our approach adaptively adjusts the exploration space, allocating more learning capacity to more challenging data and preventing the model from over-optimizing on simpler data.
                        </p> -->
                    </td>
            </tr>

                <tr onmouseout="easy_jailbreak_stop()" onmouseover="easy_jailbreak_start()">
                    <!-- <td style="padding:20px;width:30%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='easy_jailbreak_image'>
                                <img src='images/easy_jailbreak.jpg' width="190">
                            </div>
                            <img src='images/easy_jailbreak.jpg' width="190">
                        </div>
                        <script type="text/javascript">
                            function easy_jailbreak_start() {
                                document.getElementById('easy_jailbreak_image').style.opacity = "1";
                            }

                            function easy_jailbreak_stop() {
                                document.getElementById('easy_jailbreak_image').style.opacity = "0";
                            }

                            easy_jailbreak_stop()
                        </script>
                    </td> -->
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2403.12171">
                            <span class="papertitle">EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models</span>
                        </a>
                        <br>
                        Weikang Zhou, Xiao Wang, Limao Xiong, Han Xia, Yingshuang Gu, Mingxu Chai, Fukang Zhu, Caishuang Huang, Shihan Dou, <strong>Zhiheng Xi</strong>, Rui Zheng, Songyang Gao, Yicheng Zou, Hang Yan, Yifan Le, Ruohui Wang, Lijun Li, Jing Shao, Tao Gui, Qi Zhang, Xuanjing Huang

                        <br>
                        <em>Preprint.</em> Mar, 2024.
                        <br>
                        <a href="https://github.com/EasyJailbreak/EasyJailbreak?tab=readme-ov-file">project page</a>
                        /
                        <a href="https://arxiv.org/abs/2403.12171">paper</a>
                        <!-- <p></p>
                        <p>
                        EasyJailbreak is an easy-to-use Python framework designed for researchers and developers focusing on LLM security. Specifically, EasyJailbreak decomposes the mainstream jailbreaking process into several iterable steps:
                            initialize mutation seeds, select suitable seeds, add constraint, mutate, attack, and evaluate.
                            On this basis, EasyJailbreak provides a component for each step, constructing a playground for further research and attempts. More details can be found in our paper.
                        </p> -->
                    </td>
            </tr>


                <tr onmouseout="rlhf_rm_stop()" onmouseover="rlhf_rm_start()">
                    <!-- <td style="padding:20px;width:30%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='rlhf_rm_image'>
                                <img src='images/rlhf_rm.jpg' width="190">
                            </div>
                            <img src='images/rlhf_rm.jpg' width="190">
                        </div>
                        <script type="text/javascript">
                            function rlhf_rm_start() {
                                document.getElementById('rlhf_rm_image').style.opacity = "1";
                            }

                            function rlhf_rm_stop() {
                                document.getElementById('rlhf_rm_image').style.opacity = "0";
                            }

                            rlhf_rm_stop()
                        </script>
                    </td> -->
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2401.06080">
                            <span class="papertitle">Secrets of RLHF in Large Language Models Part II: Reward Modeling</span>
                        </a>
                        <br>
                        Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, Songyang Gao, Nuo Xu, Yuhao Zhou, Xiaoran Fan, <strong>Zhiheng Xi</strong>, Jun Zhao, Xiao Wang, Tao Ji, Hang Yan, Lixing Shen, Zhan Chen, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, Yu-Gang Jiang

                        <br>
                        <em>Preprint.</em> Jan, 2024.
                        <br>
                        <a href="https://arxiv.org/abs/2401.06080">codes</a>
                        /
                        <a href="https://arxiv.org/abs/2401.06080">paper</a>
                        <!-- <p></p>
                        <p>
                        In this report, we attempt to address issues of reward modeling in RLHF of LLMs.
                            (1) From a data perspective, we propose a method to measure the strength of preferences within the data,
                            based on a voting mechanism of multiple reward models.
                            (2) From an algorithmic standpoint, we introduce contrastive learning to enhance the ability of reward models to distinguish between chosen and rejected responses, thereby improving model generalization. Furthermore, we employ meta-learning to enable the reward model to maintain the ability to differentiate subtle differences in out-of-distribution samples.
                        </p> -->
                    </td>
                </tr>

                <tr onmouseout="rlhf_ppo_stop()" onmouseover="rlhf_ppo_start()">
                    <!-- <td style="padding:20px;width:30%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='rlhf_ppo_image'>
                                <img src='images/rlhf_ppo.jpg' width="190">
                            </div>
                            <img src='images/rlhf_ppo.jpg' width="190">
                        </div>
                        <script type="text/javascript">
                            function rlhf_ppo_start() {
                                document.getElementById('rlhf_ppo_image').style.opacity = "1";
                            }

                            function rlhf_ppo_stop() {
                                document.getElementById('rlhf_ppo_image').style.opacity = "0";
                            }

                            rlhf_ppo_stop()
                        </script>
                    </td> -->
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2307.04964">
                            <span class="papertitle">Secrets of RLHF in Large Language Models Part I: PPO</span>
                        </a>
                        <br>
                        Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, Limao Xiong, Lu Chen, <strong>Zhiheng Xi</strong>, Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang, Zhangyue Yin,
                        Rongxiang Weng, Wensen Cheng, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang
                        <br>
                        <em>Preprint.</em> July, 2023.
                        <br>
                        <a href="https://arxiv.org/abs/2307.04964">codes</a>
                        /
                        <a href="https://arxiv.org/abs/2307.04964">paper</a>
                        <!-- <p></p>
                        <p>
                            In this technical report, we dissect the framework of RLHF, re-evaluate the inner workings of PPO,
                            and explore how the parts comprising PPO algorithms impact policy agent training. We identify policy constraints being the key factor for the effective implementation of the PPO algorithm. Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model.
                            Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.
                        </p> -->
                    </td>
                </tr>

                <tr onmouseout="rocoins_stop()" onmouseover="rocoins_start()">
                    <!-- <td style="padding:20px;width:30%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='rocoins_image'>
                                <img src='images/rocoins.jpg' width="190">
                            </div>
                            <img src='images/rocoins.jpg' width="190">
                        </div>
                        <script type="text/javascript">
                            function rocoins_start() {
                                document.getElementById('rocoins_image').style.opacity = "1";
                            }

                            function rocoins_stop() {
                                document.getElementById('rocoins_image').style.opacity = "0";
                            }

                            rocoins_stop()
                        </script>
                    </td> -->
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2402.16431">
                            <span class="papertitle">RoCoIns: Enhancing Robustness of Large Language Models through Code-Style Instructions</span>
                        </a>
                        <br>
                        Yuansen Zhang, Xiao Wang, <strong>Zhiheng Xi</strong>, Han Xia, Tao Gui, Qi Zhang, Xuanjing Huang
                        <br>
                        <em>COLING 2024.</em>
                        <br>
<!--                        <a href="https://github.com/EasyJailbreak/EasyJailbreak?tab=readme-ov-file">codes</a>-->
<!--                        /-->
                        <a href="https://arxiv.org/abs/2402.16431">paper</a>
                        <!-- <p></p>
                        <p>
                        In this paper,
                            we utilize instructions in code style, which are more structural and less ambiguous, to replace typically natural language instructions. Through this conversion, we provide LLMs with more precise instructions and strengthen the robustness of LLMs. Moreover, under few-shot scenarios, we propose a novel method to compose in-context demonstrations using both clean and adversarial samples
                            ({adversarial context method}) to further boost the robustness of the LLMs.
                        </p> -->
                    </td>
            </tr>



                <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <tr>
                        <td style="padding:20px;width:100%;vertical-align:middle">
                            <!--            <heading>Miscellanea</heading>-->
                                                  <h2>Miscellanea</h2>

                            <ul>
                                <li> I'm passionate about FPS games, including Counter-Strike: Global Offensive (CS:GO / CS2)
                                    and CrossFire (CF).
                                </li>
                                <li> I love watching soccer and am a big fan of Mourinho.</li>
                                <li> I also love watching basketball games and my favorite player is Kevin Durant.</li>

                            </ul>
                        </td>
                    </tr>
                    </tbody>
                </table>


                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tbody>
                    <!-- <tr>
                      <td>
                        <h2>Miscellanea</h2>
                      </td>
                    </tr>
                  </tbody></table>
                  <table width="100%" align="center" border="0" cellpadding="20"><tbody>

                    <tr>
                      <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
                      <td width="75%" valign="center">
                        <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                  <br>
                        <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                        <br>
                        <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member, CVPR 2021</a>
                        <br>
                        <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                        <br>
                        <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
                      </td>
                    </tr>
                    <tr>
                      <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/cs188.jpg" alt="cs188">
                      </td>
                      <td width="75%" valign="center">
                        <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
                        <br>
                        <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
                        <br>
                        <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
                      </td>
                    </tr> -->


                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                        <tr>
                            <td style="padding:20px">
                                <p font-size:small;="">
                                    <br>
                                    <br>
                                </p>
                                <div style="float:left;">
                                    Updated at Nov 2024
                                </div>
                                <div style="float:right;">
                                    Thanks <a href="https://jonbarron.info">Jon Barron</a> for this fantastic template!
                                </div>
                                <br>
                                <br>
                                <p></p>
                            </td>
                        </tr>
                        </tbody>
                    </table>
                    </td>
                    </tr>
                </table>
</body>
</html>
